{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9374fcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Unversity\\KeyValueMemory\\kv_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54690a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"antareepdey/Patient_doctor_chat\")\n",
    "datasets = [i['Text'] for i in data['train']]\n",
    "patient_query = []\n",
    "doctor_response = []\n",
    "patient_query_val = []\n",
    "doctor_response_val = []\n",
    "\n",
    "for i in range(1000):\n",
    "    inputs, outputs = datasets[i].split(\"###Output:\")\n",
    "    patient_query.append(inputs.replace(\"###Input:\",\"\"))\n",
    "    doctor_response.append(outputs)\n",
    "for i in range(1000,1200):\n",
    "    inputs, outputs = datasets[i].split(\"###Output:\")\n",
    "    patient_query_val.append(inputs.replace(\"###Input:\",\"\"))\n",
    "    doctor_response_val.append(outputs)\n",
    "\n",
    "data = {\"Patient query\": patient_query, \"Doctor response\": doctor_response}\n",
    "val_data = {\"Patient query\": patient_query_val, \"Doctor response\": doctor_response_val}\n",
    "df = pd.DataFrame(data=data)\n",
    "val_df = pd.DataFrame(data=val_data)\n",
    "def preprocess_data(text):\n",
    "    # preprocess\n",
    "    text = text.lower()\n",
    "    # text = text.replace('?','')\n",
    "    # text = text.replace(\"'\",\"\")\n",
    "    # text = text.replace(\"1)\",\" \")\n",
    "    # text = text.replace(\"2)\",\" \")\n",
    "    # text = text.replace(\"3)\",\" \")\n",
    "    # text = text.replace(\"4)\",\" \")\n",
    "    # text = text.replace(\".\",\" \")\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "df['Patient query'] = df['Patient query'].apply(preprocess_data)\n",
    "df['Doctor response'] = df['Doctor response'].apply(preprocess_data)\n",
    "\n",
    "val_df['Patient query'] = val_df['Patient query'].apply(preprocess_data)\n",
    "val_df['Doctor response'] = val_df['Doctor response'].apply(preprocess_data)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen3-Embedding-0.6B', padding_side='left')\n",
    "embed_model = AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4cbca8",
   "metadata": {},
   "source": [
    "# make own tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e08413cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_data = patient_query + doctor_response + patient_query_val + doctor_response_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "087461c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41625af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "used_token_ids = set()\n",
    "for target_text in all_text_data:\n",
    "    tokens = tokenizer.encode(target_text, add_special_tokens=False)\n",
    "    used_token_ids.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431b97ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\n",
    "    'bos_token': '<sos>'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84485f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_ids = [tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id]\n",
    "used_token_ids.update([tid for tid in special_ids if tid is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3ec29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id_list = sorted(list(used_token_ids))\n",
    "old2new = {old: new for new, old in enumerate(token_id_list)}\n",
    "new2old = {v: k for k, v in old2new.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8a406",
   "metadata": {},
   "source": [
    "#custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a314fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.max_length = 8192\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        question = df.iloc[index]['Patient query']\n",
    "        answer = df.iloc[index]['Doctor response']\n",
    "\n",
    "        question_ids = tokenizer(\n",
    "            question,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0] \n",
    "\n",
    "        answer_ids = tokenizer(\n",
    "            answer,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "             add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )['input_ids'][0]\n",
    "\n",
    "        return question_ids, answer_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac15a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    questions, answers = zip(*batch) \n",
    "\n",
    "    padded_questions = pad_sequence(questions, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    padded_answers = pad_sequence(answers, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "\n",
    "    return padded_questions, padded_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d546c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(df) \n",
    "val_dataset = CustomDataset(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8aa7efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader( dataset, batch_size=1, shuffle=True)\n",
    "val_loader = DataLoader( val_dataset, batch_size=1,  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94d00f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892b0b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoderWithKBAttention(nn.Module):\n",
    "    def __init__(self, embed_model, vocab_size, hidden_dim, embedding_dim=1024):\n",
    "        super().__init__()\n",
    "        self.embed_model = embed_model\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.decoder = nn.LSTMCell(embedding_dim, hidden_dim * 2)\n",
    "\n",
    "        # Attention over encoder outputs\n",
    "        self.W_denc1 = nn.Linear(hidden_dim * 4, hidden_dim)\n",
    "        self.W_denc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.w = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Attention over KB keys (commented out, unused)\n",
    "        # self.W_kb1 = nn.Linear(hidden_dim + embedding_dim, hidden_dim)\n",
    "        # self.W_kb2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # self.r = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        # Final vocab projection\n",
    "        self.U = nn.Linear(hidden_dim * 4, vocab_size)\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def decoder_attention(self, decoder_hidden, encoder_hidden):\n",
    "        B, T, H = encoder_hidden.shape  # B=batch, T=encoder seq len, H=hidden size\n",
    "        decoder_exp = decoder_hidden.unsqueeze(1).expand(-1, T, -1)  # (B, T, H)\n",
    "        combined = torch.cat([encoder_hidden, decoder_exp], dim=2)  # (B, T, 2H)\n",
    "        x = torch.tanh(self.W_denc1(combined))                      # (B, T, H)\n",
    "        x = torch.tanh(self.W_denc2(x))                             # (B, T, H)\n",
    "        u = self.w(x).squeeze(-1)                                   # (B, T)\n",
    "        attn = torch.softmax(u, dim=1)                              # (B, T)\n",
    "\n",
    "        context = torch.bmm(attn.unsqueeze(1), encoder_hidden).squeeze(1)  # (B, H)\n",
    "        concat = torch.cat([decoder_hidden, context], dim=1)          # (B, 2H)\n",
    "        vocab_logits = self.U(concat)                                 # (B, vocab_size)\n",
    "        return vocab_logits\n",
    "\n",
    "    def forward(self, inputs, targets, inference=False, teacher_forcing_ratio=1.0, kb_keys=None, fine_tune=False):\n",
    "    \n",
    "\n",
    "        batch_size, T_out, _ = targets.shape\n",
    "\n",
    "        enc_out, (h_enc, c_enc) = self.encoder(inputs)  # enc_out: (B, T_in, H)\n",
    "        hidden = torch.cat([h_enc[0], h_enc[-1]], dim=-1)  # (B, hidden_dim*2)\n",
    "        cell = torch.cat([c_enc[0], c_enc[-1]], dim=-1)    # (B, hidden_dim*2)\n",
    "\n",
    "        logits = []\n",
    "\n",
    "        dec_input = targets[:, 0, :]  # Initial decoder input (usually <sos> embedding)\n",
    "\n",
    "        if inference:\n",
    "            # For inference, decode only one step here\n",
    "            hidden_state, cell_state = self.decoder(dec_input, (hidden, cell))\n",
    "            hidden_logits = self.decoder_attention(hidden_state, enc_out)\n",
    "            logits.append(hidden_logits)\n",
    "            return torch.stack(logits, dim=1)\n",
    "\n",
    "        else:\n",
    "            for t in range(1, T_out):\n",
    "                hidden_state, cell_state = self.decoder(dec_input, (hidden, cell))\n",
    "                hidden = hidden_state\n",
    "                cell = cell_state\n",
    "\n",
    "                hidden_logits = self.decoder_attention(hidden_state, enc_out)\n",
    "                logits.append(hidden_logits)\n",
    "\n",
    "                # Decide whether to do teacher forcing this step\n",
    "                use_teacher_forcing = (torch.rand(1).item() < teacher_forcing_ratio)\n",
    "\n",
    "                if use_teacher_forcing:\n",
    "                    # Use ground-truth target embedding for next input\n",
    "                    dec_input = targets[:, t, :]\n",
    "                else:\n",
    "                    # Use model prediction:\n",
    "                    pred_tokens = torch.argmax(hidden_logits, dim=1)  # (B,)\n",
    "                    with torch.no_grad():\n",
    "                        # Get embeddings for predicted tokens\n",
    "                        # Assuming embed_model accepts token IDs and returns embeddings\n",
    "                        dec_input = self.embed_model(pred_tokens)['last_hidden_state'][:, 0, :]\n",
    "\n",
    "        return torch.stack(logits, dim=1)  # (B, T_out-1, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652500de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(token_id_list)\n",
    "model = EncoderDecoderWithKBAttention(embed_model=embed_model, vocab_size=vocab_size, hidden_dim=320, embedding_dim=1024).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a767dff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "maxlength = 8192\n",
    "start_token = tokenizer(\n",
    "        '<|im_start|>',\n",
    "        max_length= maxlength,\n",
    "        return_tensors=\"pt\",\n",
    "    )['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae9b129a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644, 151643]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "110392b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3Model(\n",
       "  (embed_tokens): Embedding(151669, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x Qwen3DecoderLayer(\n",
       "      (self_attn): Qwen3Attention(\n",
       "        (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "        (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "        (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "      )\n",
       "      (mlp): Qwen3MLP(\n",
       "        (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "      (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "      (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "  (rotary_emb): Qwen3RotaryEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9568237",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embed_model =  AutoModel.from_pretrained('Qwen/Qwen3-Embedding-0.6B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "297eb140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 173, 151669])\n",
      "torch.Size([8, 174])\n"
     ]
    }
   ],
   "source": [
    "for input, target in train_loader:\n",
    "    input = new_embed_model(input)['last_hidden_state']\n",
    "    #print(input.shape)\n",
    "    #print(target.shape)\n",
    "    batch_size = target.shape[0]\n",
    "    target = torch.cat([start_token[:,0].repeat(batch_size, 1),target] , dim=1)\n",
    "    #print(target.shape)\n",
    "    outputs = new_embed_model(target)['last_hidden_state']\n",
    "    #print(outputs)\n",
    "    #print(outputs.shape)\n",
    "    #print(input.shape)\n",
    "    #print(input)\n",
    "    input = input.to(device)\n",
    "    outputs = outputs.to(device)\n",
    "    target = target.to(device)\n",
    "    logits =  model(input, outputs)\n",
    "    # #print(target[0][1:].unsqueeze(0).shape)\n",
    "    print(logits.shape)\n",
    "    print(target.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a635eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 25\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#weight_decay = 1e-2\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3969db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  19%|█▉        | 191/1000 [01:36<06:20,  2.12it/s]"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "monitor_csv = 'monitor.csv'\n",
    "\n",
    "# Write CSV header\n",
    "with open(monitor_csv, mode='w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'train_accuracy', 'val_accuracy'])\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    total_train_correct = 0\n",
    "    total_train_tokens = 0\n",
    "\n",
    "    for input_texts, target_tokens in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
    "        batch_size = target_tokens.size(0)\n",
    "\n",
    "        # Prepare inputs\n",
    "        input_texts = input_texts[:, :-1]\n",
    "        target_with_bos = torch.cat([start_token[:, 0].repeat(batch_size, 1), target_tokens], dim=1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_embeds = new_embed_model(input_texts)['last_hidden_state']\n",
    "            target_embeds = new_embed_model(target_with_bos)['last_hidden_state']\n",
    "\n",
    "        input_embeds = input_embeds.to(device)\n",
    "        target_embeds = target_embeds.to(device)\n",
    "        target_tokens = target_tokens.to(device)\n",
    "\n",
    "        # Map target_tokens to reduced vocab IDs\n",
    "        target_tokens_np = target_tokens.cpu().numpy()\n",
    "        target_reduced_np = np.vectorize(lambda x: old2new.get(x, 0))(target_tokens_np)\n",
    "        target_reduced = torch.tensor(target_reduced_np, dtype=torch.long, device=target_tokens.device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_embeds, target_embeds)\n",
    "\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.shape[-1]),\n",
    "            target_reduced.reshape(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        correct = (preds == target_reduced).float()\n",
    "        total_train_correct += correct.sum().item()\n",
    "        total_train_tokens += target_reduced.numel()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = total_train_correct / total_train_tokens\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    total_val_correct = 0\n",
    "    total_val_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_texts, target_tokens in tqdm(val_loader, desc=\"Validating\"):\n",
    "            batch_size = target_tokens.size(0)\n",
    "            target_with_bos = torch.cat([start_token[:, 0].repeat(batch_size, 1), target_tokens], dim=1)\n",
    "\n",
    "            input_embeds = new_embed_model(input_texts.to(device))['last_hidden_state']\n",
    "            target_embeds = new_embed_model(target_with_bos.to(device))['last_hidden_state']\n",
    "\n",
    "            input_embeds = input_embeds.to(device)\n",
    "            target_embeds = target_embeds.to(device)\n",
    "            target_tokens = target_tokens.to(device)\n",
    "\n",
    "            # Map target tokens to reduced vocab IDs\n",
    "            target_tokens_np = target_tokens.cpu().numpy()\n",
    "            target_reduced_np = np.vectorize(lambda x: old2new.get(x, 0))(target_tokens_np)\n",
    "            target_reduced = torch.tensor(target_reduced_np, dtype=torch.long, device=target_tokens.device)\n",
    "\n",
    "            logits = model(input_embeds, target_embeds)\n",
    "\n",
    "            loss = criterion(\n",
    "                logits.view(-1, logits.shape[-1]),\n",
    "                target_reduced.reshape(-1)\n",
    "            )\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "            correct = (preds == target_reduced).float()\n",
    "            total_val_correct += correct.sum().item()\n",
    "            total_val_tokens += target_reduced.numel()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = total_val_correct / total_val_tokens\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Save model checkpoint\n",
    "    torch.save(model.state_dict(), f\"../new_models/model_weights_{epoch:02d}.pth\")\n",
    "\n",
    "    # Save metrics to CSV\n",
    "    with open(monitor_csv, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([epoch + 1, avg_train_loss, avg_val_loss, train_accuracy, val_accuracy])\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7093905",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlength = 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "646d2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14990, 10668,   600,   614, 18873,   315,   279,  1136,  4640,   278,\n",
      "          6869,   220,   518,   279,  6869,  3704,   220,  1052,   374,  1045,\n",
      "          4158, 27575,   220,  4486,  4190, 15712,   369,   279,  3491]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "input = \"Hello doctor,I have infection of the scrotal hair. At the hair root, there is some white bump. Please suggest medicine for the problem.\"\n",
    "input = preprocess_data(input)\n",
    "start_token = tokenizer(\n",
    "        '<|im_start|>',\n",
    "        max_length= maxlength,\n",
    "        return_tensors=\"pt\",\n",
    "    )['input_ids'][:,0].unsqueeze(0).to(device)\n",
    "input_tokens = tokenizer(\n",
    "        input,\n",
    "        max_length= maxlength,\n",
    "        return_tensors=\"pt\",\n",
    "    )['input_ids'][:, :-1].to(device)\n",
    "\n",
    "print(input_tokens)\n",
    "embed_model = embed_model.to(device)\n",
    "\n",
    "embed_query = embed_model(input_tokens)['last_hidden_state'].to(device)\n",
    "target = embed_model(start_token)['last_hidden_state'].to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3bc5223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load(\"../models/model_weights_024.pth\")\n",
    "load_result = model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cea9d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "effff484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66efb32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151644"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a19a44ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_query' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m token \u001b[38;5;241m=\u001b[39m start_token  \u001b[38;5;66;03m# integer token ID for <sos>\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 1. Encode input sequence once\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     enc_out, (h_enc, c_enc) \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(\u001b[43membed_query\u001b[49m)\n\u001b[0;32m      8\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h_enc[\u001b[38;5;241m0\u001b[39m], h_enc[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 2H)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     cell   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([c_enc[\u001b[38;5;241m0\u001b[39m], c_enc[\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, 2H)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_query' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "tokens = []\n",
    "token = start_token  # integer token ID for <sos>\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 1. Encode input sequence once\n",
    "    enc_out, (h_enc, c_enc) = model.encoder(embed_query)\n",
    "    hidden = torch.cat([h_enc[0], h_enc[1]], dim=-1)  # (B, 2H)\n",
    "    cell   = torch.cat([c_enc[0], c_enc[1]], dim=-1)  # (B, 2H)\n",
    "\n",
    "    # 2. Start with <sos> token embedding\n",
    "    input_emb = embed_model(torch.tensor([[token]]).to(device))['last_hidden_state'][:, 0, :]  # (1, E)\n",
    "\n",
    "    for i in range(20):  # max decoding steps\n",
    "        # One LSTMCell step\n",
    "        hidden, cell = model.decoder(input_emb, (hidden, cell))\n",
    "\n",
    "        # Attention over encoder outputs\n",
    "        vocab_logits = model.decoder_attention(hidden, enc_out)\n",
    "\n",
    "        # Predict next token\n",
    "        pred_token = torch.argmax(vocab_logits, dim=1).item()\n",
    "        tokens.append(pred_token)\n",
    "\n",
    "        # Stop if EOS token predicted\n",
    "        if pred_token == 151644:\n",
    "            break\n",
    "\n",
    "        # Embed predicted token for next step\n",
    "        input_emb = embed_model(torch.tensor([[pred_token]]).to(device))['last_hidden_state'][:, 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83968fdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6023,\n",
       " 1850,\n",
       " 2518,\n",
       " 1850,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518,\n",
       " 2518]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a1af6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12649b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'Ġbest',\n",
       " 'Ġred',\n",
       " 'Ġbest',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred',\n",
       " 'Ġred']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41efd68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = torch.argmax(logits[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c65e0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41e556af",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list.append(token.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a4b07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([9330], device='cuda:0'), 9330]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43149cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPast(last_hidden_state=tensor([[[  2.3688, -15.1576,  -0.0910,  ...,  -7.9240, -11.6597,   0.9951]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>), past_key_values=<transformers.cache_utils.DynamicCache object at 0x00000201FF880190>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_model(token.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42db4be8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc9b8c58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(9330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a648df01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "649c8adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644, 151643]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fde2d4e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.8472,  -4.0913,   0.2632,  ...,  -4.2332, -10.3624,   0.6719]]],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86769c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:  12%|█▏        | 124/1000 [01:02<07:25,  1.97it/s]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a738e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = [0.8739, 0.8879, 0.9021, 0.9004, 0.9212]\n",
    "train_acc = [0.8456, 0.8663, 0.8772, 0.8773, 0.9036]\n",
    "train_loss = [0.5003, 0.4344, 0.4007, 0.3974, 0.3111]\n",
    "val_loss = [0.4197, 0.3763, 0.3406, 0.3411, 0.2604]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad367e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cf904be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7696d54e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.3917e+00,  1.2132e+00, -1.8796e-01,  ..., -8.5305e+00,\n",
       "          -1.1822e+01, -4.7525e-03],\n",
       "         [ 2.0638e-01, -4.2831e+00, -1.1676e+00,  ..., -2.1979e+00,\n",
       "          -1.1188e+00, -3.0773e+00],\n",
       "         [-2.8330e+00, -8.4577e-01, -1.0637e+00,  ..., -2.3205e+00,\n",
       "           2.6245e+00, -8.2585e-01],\n",
       "         ...,\n",
       "         [-1.7195e+00,  1.7569e+00, -7.6966e-01,  ..., -1.3342e+00,\n",
       "          -2.3326e-01, -1.9689e+00],\n",
       "         [-6.4266e+00, -5.7917e+00, -7.3048e-01,  ...,  2.4106e+00,\n",
       "          -2.4566e+00, -7.1773e+00],\n",
       "         [-1.5489e+00,  2.2582e+00, -8.7037e-01,  ...,  2.0526e+00,\n",
       "           1.2427e+00, -2.1785e+00]]], device='cuda:0',\n",
       "       grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44a652d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.8472,  -4.0913,   0.2632,  ...,  -4.2332, -10.3623,   0.6719]]],\n",
       "       device='cuda:0', grad_fn=<ToCopyBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
